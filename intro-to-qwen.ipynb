{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cc6b4a",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/acceleratescience/llms-for-pi/blob/main/intro-to-qwen.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168582a",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face\n",
    "In this notebook, we will introduce the basic code required to download a model and generate some text with it. The model that we will use is Qwen2.5-0.5B, but the process is pretty much the same for any other model.\n",
    "\n",
    "If you want to check out other models, you can head to [Hugging Face](https://huggingface.co/models) and just browse. If you sign up to Hugging Face, you can also click on most of the models, and actually chat with them (it's on the right hand side of the page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0f4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce80f97",
   "metadata": {},
   "source": [
    "The model has already been downloaded by the `setup.sh` script. If you replace the text below with another model, it will have to be downloaded, and may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ebbe8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde5f60",
   "metadata": {},
   "source": [
    "The pipeline for Hugging Face text generation has two parts:\n",
    "\n",
    "- The tokenizer\n",
    "- The model\n",
    "\n",
    "We use the inbuilt methods to load the tokenizer and the model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f97126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba7c0e",
   "metadata": {},
   "source": [
    "Remember that LLMs like a certain pattern of inputs that looks like:\n",
    "\n",
    "system &rarr; user &rarr; assistant &rarr; user &rarr; assistant &rarr; ...\n",
    "\n",
    "So we need to structure our prompts in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e5cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "system = \"You are a world-class poet.\"\n",
    "prompt = \"Give me a haiku about a Samurai cat.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff8a60e",
   "metadata": {},
   "source": [
    "We then pass these into the tokenizer's chat template. Almost all models have a chat template that matches the format of the training data. This is pretty much required to get the best out of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00c2ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a world-class poet.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a haiku about a Samurai cat.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3f582",
   "metadata": {},
   "source": [
    "But now we need to tokenize (break up) the text, and convert it to numbers. If you remember from the slides, we can think of this like breaking the text up into words and then assigning each word a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a392b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56625d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151644</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8948</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2610</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">525</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">264</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1879</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14800</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39260</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151645</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151644</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">872</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35127</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">752</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">264</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6386</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38242</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">911</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">264</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88018</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8251</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151645</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151644</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77091</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span><span style=\"font-weight: bold\">]])</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]])</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m151644\u001b[0m,   \u001b[1;36m8948\u001b[0m,    \u001b[1;36m198\u001b[0m,   \u001b[1;36m2610\u001b[0m,    \u001b[1;36m525\u001b[0m,    \u001b[1;36m264\u001b[0m,   \u001b[1;36m1879\u001b[0m,  \u001b[1;36m14800\u001b[0m,  \u001b[1;36m39260\u001b[0m,\n",
       "\u001b[2;32m│   │   │    \u001b[0m\u001b[1;36m13\u001b[0m, \u001b[1;36m151645\u001b[0m,    \u001b[1;36m198\u001b[0m, \u001b[1;36m151644\u001b[0m,    \u001b[1;36m872\u001b[0m,    \u001b[1;36m198\u001b[0m,  \u001b[1;36m35127\u001b[0m,    \u001b[1;36m752\u001b[0m,    \u001b[1;36m264\u001b[0m,\n",
       "\u001b[2;32m│   │      \u001b[0m\u001b[1;36m6386\u001b[0m,  \u001b[1;36m38242\u001b[0m,    \u001b[1;36m911\u001b[0m,    \u001b[1;36m264\u001b[0m,  \u001b[1;36m88018\u001b[0m,   \u001b[1;36m8251\u001b[0m,     \u001b[1;36m13\u001b[0m, \u001b[1;36m151645\u001b[0m,    \u001b[1;36m198\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m151644\u001b[0m,  \u001b[1;36m77091\u001b[0m,    \u001b[1;36m198\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'attention_mask'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8911f2e",
   "metadata": {},
   "source": [
    "We have two parts here:\n",
    "\n",
    "`input_ids` : the numerical form of the tokenized text.\n",
    "\n",
    "`attention_mask` : for when you want to stack multiple inputs together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c8e71",
   "metadata": {},
   "source": [
    "Now we are ready to generate some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44247892",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ff67d",
   "metadata": {},
   "source": [
    "Just as the model only understands numbers, it also only outputs numbers. We therefore need to decode the output, using out lookup table of numbers to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4da56b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squinting eyes,\n",
      "Silent claws, keen sense—\n",
      "Samurai's loyal friend.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a2a76",
   "metadata": {},
   "source": [
    "This process is pretty slow. Fortunately, there are some very efficient inference engines available to us. The one that we will try now is called Ollama..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
