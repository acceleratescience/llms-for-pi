{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69127b79",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/acceleratescience/llms-for-pi/blob/main/ollama-colab.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead69b74",
   "metadata": {},
   "source": [
    "# Running Ollama in Jupyter Colab\n",
    "This notebook is meant to be loaded in Google Colab. It is a highly unusual way to serve ollama models, but here we are...\n",
    "\n",
    "Click on the Open in Colab button at the top of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087574b",
   "metadata": {},
   "source": [
    "First, we have to load an extension called `xterm` that lets us use the terminal in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d4a65",
   "metadata": {},
   "source": [
    "Run the cell below, and then copy over the following two commands. If you want to paste into the terminal, you can use `ctrl`+`shift`+`v`.\n",
    "\n",
    "```bash\n",
    "curl https://ollama.ai/install.sh | sh\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e7226",
   "metadata": {},
   "source": [
    "You'll probably see some errors or warnings, but it's fine. Leave the above terminal to run, and start another terminal in the cell below.\n",
    "\n",
    "When it is active, run the following\n",
    "\n",
    "```bash\n",
    "ollama run qwen2.5:0.5b\n",
    "```\n",
    "And then start chatting with your model!\n",
    "\n",
    "Again, you'll probably see some warning messages, but it's all good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72576b",
   "metadata": {},
   "source": [
    "## OpenAI compatibility\n",
    "If you scroll up to the top terminal, you can find somewhere, a line that says something like:\n",
    "\n",
    "```bash\n",
    "time=2025-08-14T16:18:02.123Z level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
    "```\n",
    "\n",
    "The important part here is the\n",
    "\n",
    "```bash\n",
    "Listening on 127.0.0.1:11434\n",
    "```\n",
    "\n",
    "Ollama models are actually compatible with the OpenAI API, which means you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda19e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"qwen2.5:0.5b\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Captain Jack Sparrow.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a poem about coconuts.\"},\n",
    "  ],\n",
    "  max_tokens=256,\n",
    "  temperature=0.7,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d25b78",
   "metadata": {},
   "source": [
    "Try messing around with the `temperature` parameter. See what happens when you decrease or increase it...\n",
    "\n",
    "...try and find out why this happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
